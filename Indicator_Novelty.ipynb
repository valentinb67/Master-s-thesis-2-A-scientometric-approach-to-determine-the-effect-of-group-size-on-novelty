{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import novelpy\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openalex_data(query, num_results=1250):\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    results = []\n",
    "    per_page = 15\n",
    "    \n",
    "    for offset in range(0, num_results, per_page):\n",
    "        response = requests.get(f\"{base_url}?filter=title.search:{query},from_publication_date:2007-01-01,to_publication_date:2024-12-31&per-page={per_page}&page={offset // per_page + 1}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()['results']\n",
    "            results.extend(data)\n",
    "        else:\n",
    "            print(f\"Erreur : {response.status_code}\")\n",
    "            break\n",
    "        if len(data) < per_page:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_int_id(string_id):\n",
    "    \"\"\"Generate a unique integer ID from a string ID using hashing\"\"\"\n",
    "    return int(hashlib.sha256(string_id.encode('utf-8')).hexdigest(), 16) % (10**8)\n",
    "\n",
    "def prepare_data_for_novelpy(data):\n",
    "    prepared_data = []\n",
    "    for item in data:\n",
    "        try:\n",
    "            if 'id' not in item:\n",
    "                print(f\"Skipping item without id: {item}\")\n",
    "                continue\n",
    "\n",
    "            year = item.get('publication_year', None)\n",
    "            if year is None or not (2007 <= year <= 2024):\n",
    "                print(f\"Skipping item outside date range: {item['id']}\")\n",
    "                continue\n",
    "\n",
    "            authorships = item.get('authorships', [])\n",
    "            authors = [author['author']['display_name'] for author in authorships]\n",
    "            institutions = []\n",
    "            for author in authorships:\n",
    "                if 'institutions' in author:\n",
    "                    for inst in author['institutions']:\n",
    "                        institutions.append(inst.get('display_name', ''))\n",
    "\n",
    "            entry = {\n",
    "                \"PMID\": generate_int_id(item['id']),\n",
    "                \"year\": year,\n",
    "                \"type\": item.get('type', ''),\n",
    "                \"num_citations\": item.get('cited_by_count', 0),\n",
    "                \"num_authors\": len(authorships),\n",
    "                \"authors\": authors,\n",
    "                \"institutions\": institutions,\n",
    "                \"c04_referencelist\": [{\"item\": generate_int_id(ref)} for ref in item.get('referenced_works', [])],\n",
    "                \"subfield\": item.get('concepts', [{}])[0].get('display_name', '') if item.get('concepts') else '',\n",
    "                \"field\": item.get('concepts', [{}])[1].get('display_name', '') if len(item.get('concepts', [])) > 1 else '',\n",
    "                \"domain\": item.get('concepts', [{}])[2].get('display_name', '') if len(item.get('concepts', [])) > 2 else '',\n",
    "                \"sustainable_development_goals\": item.get('sustainable_development_goals', [])\n",
    "            }\n",
    "            prepared_data.append(entry)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e} in item {item.get('id', 'unknown')}\")\n",
    "            continue\n",
    "    return prepared_data\n",
    "\n",
    "def save_data_by_year(prepared_data, base_dir='Data/docs/references_sample'):\n",
    "    data_by_year = {}\n",
    "    for item in prepared_data:\n",
    "        year = item['year']\n",
    "        if year not in data_by_year:\n",
    "            data_by_year[year] = []\n",
    "        data_by_year[year].append(item)\n",
    "    \n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for year, data in data_by_year.items():\n",
    "        with open(os.path.join(base_dir, f\"{year}.json\"), 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "def validate_data(prepared_data):\n",
    "    all_ids = set(item['PMID'] for item in prepared_data)\n",
    "    valid = True\n",
    "    for item in prepared_data:\n",
    "        for ref in item['c04_referencelist']:\n",
    "            if ref['item'] not in all_ids:\n",
    "                print(f\"Reference {ref['item']} in document {item['PMID']} does not exist in the dataset.\")\n",
    "                valid = False\n",
    "    return valid\n",
    "\n",
    "query = \"sustainable development goals\"\n",
    "data = get_openalex_data(query, num_results=1250)\n",
    "prepared_data = prepare_data_for_novelpy(data)\n",
    "save_data_by_year(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la matrice de cooccurence et de l'indicateur de Lee et al. (2015):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_cooc = novelpy.utils.cooc_utils.create_cooc(\n",
    "    collection_name=\"references_sample\",\n",
    "    year_var=\"year\",\n",
    "    var=\"c04_referencelist\",\n",
    "    sub_var=\"item\",\n",
    "    time_window=range(2007, 2024),\n",
    "    weighted_network=True, \n",
    "    self_loop=True\n",
    ")\n",
    "ref_cooc.main()\n",
    "\n",
    "focal_years = range(2007, 2024)\n",
    "collection_name = 'references_sample'\n",
    "id_variable = 'PMID'\n",
    "year_variable = 'year'\n",
    "variable = 'c04_referencelist'\n",
    "sub_variable = 'item'\n",
    "\n",
    "for focal_year in tqdm.tqdm(focal_years, desc=\"Computing Lee indicator for window of time\"):\n",
    "    Lee = novelpy.indicators.Lee2015(\n",
    "        collection_name=collection_name,\n",
    "        id_variable=id_variable,\n",
    "        year_variable=year_variable,\n",
    "        variable=variable,\n",
    "        sub_variable=sub_variable,\n",
    "        focal_year=focal_year,\n",
    "        density=True\n",
    "    )\n",
    "    Lee.get_indicator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = novelpy.utils.novelty_trend(\n",
    "    year_range=range(2007, 2024, 1),\n",
    "    variables=[\"c04_referencelist\"],\n",
    "    id_variable=\"PMID\",\n",
    "    indicators=[\"lee\"]\n",
    ")\n",
    "\n",
    "trend.get_plot_trend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération sous format csv du commonness\n",
    "def convert_to_dataframe(data):\n",
    "    records = []\n",
    "    for item in data:\n",
    "        pmid = item.get('PMID', None)\n",
    "        year = item.get('year', None)\n",
    "        novelty_score = item.get('c04_referencelist_lee', {}).get('score', {}).get('novelty', None)\n",
    "        records.append({\n",
    "            'PMID': pmid,\n",
    "            'Year': year,\n",
    "            'Commonness_Score': novelty_score\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def load_data_from_files(start_year, end_year, directory):\n",
    "    all_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        file_path = os.path.join(directory, f\"{year}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.extend(data)\n",
    "                print(f\"Data from {year}.json loaded successfully\")\n",
    "        else:\n",
    "            print(f\"File {file_path} does not exist\")\n",
    "    return all_data\n",
    "\n",
    "def convert_to_dataframe(data):\n",
    "    records = []\n",
    "    for item in data:\n",
    "        pmid = item.get('PMID', None)\n",
    "        novelty_score = item.get('c04_referencelist_lee', {}).get('score', {}).get('novelty', None)\n",
    "        records.append({\n",
    "            'PMID': pmid,\n",
    "            'Commonness Score': novelty_score\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Load data for Lee\n",
    "start_year = 2007\n",
    "end_year = 2024\n",
    "directory = 'Result/lee/c04_referencelist/'\n",
    "data_lee = load_data_from_files(start_year, end_year, directory)\n",
    "print(f\"Total records loaded: {len(data_lee)}\")\n",
    "lee_df = convert_to_dataframe(data_lee)\n",
    "lee_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des infos des papier en CSV\n",
    "def convert_to_dataframe(data):\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "query = \"sustainable development goals\"\n",
    "data = get_openalex_data(query, num_results=1000)\n",
    "prepared_data = prepare_data_for_novelpy(data)\n",
    "save_data_by_year(prepared_data)\n",
    "\n",
    "# Charger les données sauvegardées pour une période donnée\n",
    "def load_data_from_files(start_year, end_year, directory):\n",
    "    all_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        file_path = os.path.join(directory, f\"{year}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.extend(data)\n",
    "                print(f\"Data from {year}.json loaded successfully\")\n",
    "        else:\n",
    "            print(f\"File {file_path} does not exist\")\n",
    "    return all_data\n",
    "\n",
    "start_year = 2007\n",
    "end_year = 2024\n",
    "directory = 'Data/docs/references_sample/'\n",
    "\n",
    "data = load_data_from_files(start_year, end_year, directory)\n",
    "\n",
    "df_articles = convert_to_dataframe(data)\n",
    "df_articles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df.to_csv(\"DF_ML_Lee\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = df['type'].unique()\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.lmplot(x='Commonness Score', y='num_authors', hue='type', data=df, \n",
    "           markers=['o', 's', 'D', '^', 'v'], palette='Set1', aspect=1.5, height=6)\n",
    "plt.title('Lien entre Commonness Score et num_authors pour les différentes valeurs de type')\n",
    "plt.xlabel('Commonness Score')\n",
    "plt.ylabel('Number of Authors')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_commonness_trend(df, title='Trend of Commonness Scores Over Time'):\n",
    "    trend_data = df.groupby('Year')['Commonness Score'].mean().reset_index()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(trend_data['Year'], trend_data['Commonness Score'], marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Average Commonness Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_commonness_trend(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_commonness_distribution(df, title='Distribution of Commonness Scores'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df['Commonness Score'], bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Commonness Score')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    plt.show()\n",
    "\n",
    "plot_commonness_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_commonness_boxplot(df, title='Commonness Scores by Year'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Year', y='Commonness Score', data=df)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Commonness Score')\n",
    "    plt.show()\n",
    "\n",
    "plot_commonness_boxplot(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novelpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
